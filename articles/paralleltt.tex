% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage[utf8]{inputenc}
\usepackage[cmex10,fleqn]{amsmath}
\usepackage{amsfonts,amssymb}
%%\usepackage{amsfonts,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{todonotes}
%%\usepackage{algorithm,algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmic}
%%\usepackage{algcompatible}
\usepackage{subfig}
\usepackage{xspace}
\usepackage{etoolbox}
\usepackage{xcolor}
\usepackage{multirow}
\setlength{\tabcolsep}{3pt}

\definecolor{pdfurlcolor}{rgb}{0,0,0.6}
\definecolor{pdfcitecolor}{rgb}{0,0.6,0}
\definecolor{pdflinkcolor}{rgb}{0.6,0,0}

\usepackage[colorlinks=true,citecolor=pdfcitecolor,urlcolor=pdfurlcolor,linkcolor=pdflinkcolor,pdfborder={0
	0 0}]{hyperref}
%%\usepackage[breaklinks]{hyperref}
%%\usepackage{subcaption}

\usepackage{tikz}
\usetikzlibrary{matrix,snakes, patterns, positioning, shapes, calc, intersections, arrows, fit}

\graphicspath{{./diagrams/}}

%%\theoremstyle{plain}
%%\newtheorem{lemma}{Lemma}
%%\newtheorem{theorem}{Theorem}
%%\newtheorem{proposition}{Proposition}
%%\newtheorem{corollary}{Corollary}
%%\newtheorem{definition}{Definition}

%%\DeclareMathOperator{\FRT}{FRT}
%%\newcommand{\heteroprioD}{{HeteroPrioDep}\xspace}
\newcommand{\tensor}[1]{\cal\textbf{#1}\xspace}
\newcommand{\ttrain}{{\it Tensor-Train}\xspace}


% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Parallel Algorithms for Tensor-Train Decomposition}
%%%%\title{Contribution Title\thanks{Supported by organization x.}}
%%%%%
%%%%%\titlerunning{Abbreviated paper title}
%%%%% If the paper title is too long for the running head, you can set
%%%%% an abbreviated paper title here
%%%%%
\author{}
\institute{}
%%%%\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
%%%%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%%%%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%%%%%
%%%%\authorrunning{F. Author et al.}
%%%%% First names are abbreviated in the running head.
%%%%% If there are more than two authors, 'et al.' is used.
%%%%%
%%%%\institute{Princeton University, Princeton NJ 08544, USA \and
%%%%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%%%%\email{lncs@springer.com}\\
%%%%\url{http://www.springer.com/gp/computer-science/lncs} \and
%%%%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%%%%\email{\{abc,lncs\}@uni-heidelberg.de}}
%%%%%
\maketitle              % typeset the header of the contribution

\begin{abstract}
The abstract should briefly summarize the contents of the paper in
150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}

\section{Introduction}
\label{sec:introduction}
% 1-page
\begin{itemize}
	\item Popularity of tensors
	\item Need towards developing parallel and communication efficient algorithms
	\item HPC supercomputers
	\item tensor decomposition uses in Machine learning, Chemistry applications
	\item intuition behind Tensor train decomposition and its properties
	\item divison of indices for TT and PTT decomposition
	\item outline of this paper
\end{itemize}
\section{Related Work}
\label{sec:relatedWork}
%0.5 page
\begin{itemize}
	\item Different types of tensor decompositions
	\item Properties  and limitation of each type of tensor decomposition (Look at Osoledets's advisor work)
	\item Kolda's work
	\item Ballard's work
	\item Solomonik's work
\end{itemize}


\section{Tensor Train Decomposition}
\label{sec:tt}
% 0.5 page
\begin{itemize}
	\item Tensor train decomposition
	\item Complexity and its analysis
\end{itemize}

\noindent Some quantum molecular simulations require manipulating as high as 10 to 100 dimensional tensors. An efficient representation of a tensor with small number of variables provides opportunities to work with such high dimensional tensors. \ttrain decomposition is a popular way to obtain such a representation with few number of variables. It was proposed recently in ~\cite{tt} by \textit{Oseledets}. It represents a $d$-dimensional tensor with $2$ matrices and $d$-$2$ $3$-dimensional tensors. These are called cores of the TT-decomposition.

\begin{figure}
	\begin{center}

	\includegraphics[scale=0.05]{./tt_decomposition.jpg}
	\caption{\ttrain decomposition of a tensor. An entry of a $d$-dimensional tensor is computed by multiplying corresponding matrix (or row/column) of each tensor core, i.e., $A(i_1, i_2,\cdots, i_d) = G_1(i_1)G_2(i_2)\cdots G_d(i_d)$.\label{fig:ttdiagram}}
	\end{center}
\end{figure}

\noindent With TT-decomposition, a $d$-dimensional tensor $n_1 \times n_2 \times \cdots \times n_d$ is represented with cores $G_k$ of size $r_{k-1}\times n_k\times r_k$, $k=1,2,\cdots d$, $r_0=r_d=1$ and its elements satisfy the following expression:
\begin{align*}
A(i_1, i_2,\cdots ,i_d) &= \sum_{\alpha_0 = 1}^{r_0} \sum_{\alpha_1 = 1}^{r_1} \cdots \sum_{\alpha_d = 1}^{r_d} G_1(\alpha_0, i_1, \alpha_1) G_2(\alpha_1, i_2, \alpha_2)\cdots G_d(\alpha_{d-1}, i_1, \alpha_d)\\
&= \sum_{\alpha_1 = 1}^{r_1} \cdots \sum_{\alpha_{d-1} = 1}^{r_{d-1}} G_1(1, i_1, \alpha_1) G_2(\alpha_1, i_2, \alpha_2)\cdots G_d(\alpha_{d-1}, i_1, 1)
\end{align*}

\noindent Here $G_k(\alpha_{k-1}, i_k, \alpha_k)$ are the elements of $G_k$. Since $r_0=r_d=1$, hence $G_1$ and $G_d$ cores are matrices. Other cores are $3$-dimensional tensors.

\noindent Figure~\ref{fig:ttrainchain} exhibits that the above expression can also be represented graphically by a linear chain where sum over circle nodes (indices $\alpha_k$) are assumed to compute an entry of the tensor. This figure looks like a train, hence the \ttrain (TT) name is used for the decomposition. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.05]{./chaintt.jpg}
		\caption{Chain representation of \ttrain decomposition.\label{fig:ttrainchain}}
	\end{center}
\end{figure}

\section{Paralle Tensor Train Decomposition}
\label{sec:ptt}
% 2.5 pages
\begin{itemize}
	\item Different notations
	\item Parallel Tensor train algorithm and its proof
	\item Working principle with a matrix diagram and its reshape property
	\item Its generalization idea that we can break it from anywhere
\end{itemize}


\section{Conclusion}
\label{sec:conclusion}
\begin{itemize}
	\item Proof, heuristics and experiments
	\item Implementation of this approach
	\item RRQR instead of svd
	\item Combinatorial choices to find the best order to perform TT or PTT decomposition
\end{itemize}




%
\section{Tensor Train Decomposition}
\label{sec:tt_sequential}

Let $A(i_1,i_2,\cdots, i_d)$ denote the elements of a $d$-dimensional tensor \tensor{A}. The dimensions of \tensor{A} are $n_1 \times n_2 \times \cdots \times n_d$. The $k$-th unfolding matrix of tensor \tensor{A} is represented by $A_k$.

\begin{align*}
A_k &= A_k(i_1, i_2,\cdots, i_k; i_{k+1},\cdots ,i_d)
\end{align*}

\noindent The first $k$ indices represent the rows of $A_k$ and the last $d-k$ the columns of $A_k$. The size of this matrix is $(\prod_{l=1}^{k}n_l)\times(\prod_{l=k+1}^{d}n_l)$. The rank of this matrix is denoted by $r_k$. Let ($r_1, r_2,\cdots, r_{d-1}$)  denote the rank of each unfolding matrix of tensor \tensor{A}.

%%\noindent \textit{Oseledets} proposed an algorithm to compute Tensor Train (TT) decomposition of a tensor in ~\cite{tt}. We also describe this in Algorithm~\ref{alg:tt_original}. 

\noindent \textit{Oseledets} presented an algorithm to compute Tensor Train (TT) decomposition of a tensor. Here we present the main idea of his algorithm. Let \tensor{A} be a $d$-dimensional tensor with dimensions $n_1 \times n_2 \times \cdots \times n_d$. His algorithm operates in $d-1$ steps. In each step, one dimension of the tensor is decoupled from the remaining dimensions using singular value decomposition. In the first step, \tensor{A} is represented as a matrix of size $n_1$ $\times$ $n_2 n_3 \cdots n_d$ and svd is computed for this matrix. The left singular vectors corresponding to nonzero singular values are the first core of TT-decomposition. In the second step, interaction with the first core and the second dimension are represented as rows of the matrix while other dimensions $n_3\cdots n_d$ represent the columns. Again svd is computed for this matrix and  left singular vectors are rearranged to obtain the second  core of TT-decomposition. This process is repeated for $d-1$ steps. Hence $d-1$ cores are obtained by this process. Remaining matrix, which represents the interaction with the $d-1$th core and $n_d$, constitutes the last core of the decomposition. This algorithm returns tensor cores $G_k(\alpha_{k-1}, n_k, \alpha_k) _{1\le k\le d}$ and also ensures that $\alpha_k \le r_k$. We refer the reader to the original paper for more details about this algorithm~\cite{tt}. 

%%%% We also describe this in Algorithm~\ref{alg:tt_original}. 
%%
%%
%%%%SK: todo: remove this algorithm and express this in words
%%\begin{algorithm}[htb]
%%	\caption{\label{alg:tt_original}TT decomposition (\textcolor{green}{sequential})}
%%	\begin{algorithmic}[1]
%%		\REQUIRE $d$-dimensional tensor \tensor{A} and ranks ($r_1, r_2,\cdots, r_{d-1}$) 
%%		\ENSURE Cores $G_k(\alpha_{k-1}, n_k, \alpha_k) _{1\le k\le d}$ of the TT-decomposition with $\alpha_k \le r_k$ and $\alpha_0 = \alpha_d=1$
%%		\STATE Temporary tensor $\tensor{C}=\tensor{A}$, $\alpha_0=1$
%%		\FOR{$k=1$ \TO $d-1$} 
%%		\STATE // matrix of size $\alpha_{k-1} n_k \times n_{k+1}\cdots n_d$
%%		\STATE $C$ =  reshape($C, \alpha_{k-1} n_k$) 
%%		\STATE SVD: $C=U \Sigma V^\intercal$
%%		\STATE $\alpha_k=$ rank($\Sigma$)
%%		\STATE // first $\alpha_k$ columns of $U$
%%		%%		\COMMENT {Hello}
%%		\STATE $U_k = U(;1:\alpha_k)$
%%		\STATE Temporary $T =\Sigma V^\intercal$
%%		\STATE //first $\alpha_k$ rows of $T$
%%		\STATE $C= T(1:\alpha_k;)$
%%		\STATE $G_k$ = reshape($U_k, \alpha_{k-1}, n_k, \alpha_k$)
%%		\ENDFOR
%%		\STATE $G_d = C$, $\alpha_d=1$
%%		\RETURN tensor cores $G_k(\alpha_{k-1}, n_k, \alpha_k) _{1\le k\le d}$
%%	\end{algorithmic}
%%\end{algorithm}
%%
%%Algorithm~\ref{alg:tt_original} returns a TT decomposition of tensor \tensor{A}. It also ensures that $\alpha_k \le r_k$ $_{1\le k < d}$. We refer the reader to the original paper~\cite{tt} for more details about this algorithm. 

The \textit{Oseledets}'s TT-decomposition algorithm is sequential. It separates one dimension in each step and then operate on the remaining portion in the next step. Most modern computing platforms are composed of several number of nodes and cores. Hence running a sequential algorithm on these platform may result in poor utilization of resources. In the next section, we propose a parallel algorithm to perform tensor train decomposition which exhibits $\frac{d}{2}$-level of parallelism. We also prove that the tensor cores $G_k(\beta_{k-1}, n_k, \beta_k) _{1\le k\le d}$ produced by our algorithm satisfy $\beta_k \le r_k$ $_{1\le k < d}$ property.
  
%%The original TT decomposition algorithm is sequential. In this article, we modify the original algorithm so that it can run efficiently in parallel. The description of the modified algorithm is presented in the next section.

\section{Parallel TT Decomposition}
\label{sec:tt_parallel}

The original indices of a tensor are called external indices, while the indices arise due to SVD are called internal indices. $nE$ and $nI$ denote the number of external and internal indices respectively of a tensor. A tensor with elements $A(\alpha, i_1, i_2, i_3, \beta)$ has $3$ external and $2$ internal indices. We also extend the definition of unfolding matrix to take internal indices into account. The $k$-th unfolding of tensor \tensor{A}, whose elements are $A(\alpha, i_1, i_2, \cdots,i_k, i_{k+1}, \cdots, \beta)$, is represented as,
%%\todo[inline]{SK: define a tensor for this $A_k$ representation}
\begin{align*}
A_k &= A_k(\alpha, i_1, i_2, \cdots, i_k; i_{k+1}\cdots, \beta)
\end{align*}
Here we consider all indices from  the beginning to $i_{k}$ as the rows of $A_k$ and the remaining indices as the columns of $A_k$.
%%\todo[inline]{SK: make this function smooth for recursive calls}
\begin{algorithm}[htb]
	\caption{\label{alg:tt_parallel}TT-parallel (parallel Tensor Train Decomposition)}
	\begin{algorithmic}[1]
		\REQUIRE $d$-dimensional tensor \tensor{A} and ranks ($r_1, r_2,\cdots r_{d-1}$) 
		\ENSURE Cores $G_k(\alpha_{k-1}, n_k, \alpha_k) _{1\le k\le d}$ of the TT-decomposition with $\alpha_k \le r_k$ and $\alpha_0 = \alpha_d=1$
		\IF{$nE(\tensor{A})> $$1$} 
		\STATE Find the middle external index $k$
%%		\STATE $k=$mid-external-index(\tensor{A}) 
%%		\STATE $A_k =$ reshape(\tensor{A},k)
		\STATE Compute unfolding matrix $A_k$ 
		\STATE Compute SVD: $A_k = U \Sigma V^\intercal$
		\STATE $\alpha_k=$ rank($\Sigma$)
		\STATE Select diagonal matrices $X_k$, $S_k$ and $Y_k$ such that $X_kS_kY_k = \Sigma(1:\alpha_k; 1:\alpha_k)$\label{alg:line:xsy}
%%		\STATE // first $\alpha_k$ columns of $U$
		\STATE $\tensor{A}$$_{left}$ = Tensor($U(;1:\alpha_k)X_k$)
		\STATE list1 = TT-parallel($\tensor{A}$$_{left}$, ($r_1, r_2,\cdots r_{k-1}$) )
%%		\STATE Temporary $T =\Sigma V$
%%		\STATE //first $\alpha_k$ rows of $\Sigma V$
		\STATE $\tensor{A}$$_{right}$ = Tensor($Y_kV^\intercal(1:\alpha_k;)$)
		\STATE list2 = TT-parallel($\tensor{A}$$_{right}$, ($r_{k+1},\cdots r_{d-1}$))
		\RETURN \{list1, list2\}
%%		\ELSIF{$nE(\tensor{A})==$$1$ and $nI(\tensor{A})==$$1$} 
%%		\STATE $k=$ external-index(\tensor{A})
%%		\STATE if $k$ is the first index of $\tensor{A}$ then $\alpha_0=1$ else $\alpha_d = 1$
%%		\STATE $G_k = \tensor{A}$
%%		\RETURN $G_k$
		\ELSE 
		\STATE Find the external index $k$
		\IF {$k$ is the last index of \tensor{A}} 
		\STATE $\alpha_d = 1$
		\STATE $G_k = \tensor{A}$
		\RETURN $G_k$
		\ENDIF
		\IF {$k$ is the first index of \tensor{A}}
		\STATE $\alpha_0 = 1$
		\ENDIF
		\STATE Compute unfolding matrix $A_k$
		\STATE Update $A_k$, $A_k = A_kS_k$
		\STATE  $G_k$ = Tensor($A_k$)
		\STATE return $G_k$
		\ENDIF
	\end{algorithmic}
\end{algorithm} 

Algorithm~\ref{alg:tt_parallel} is recursive and returns the tensor cores of a TT decomposition for tensor \tensor{A}. It is possible to work directly with unfolding matrices in the above algorithm, however for the ease of presentation, intermediate unfolding matrices are converted to tensors several times. We can also note that selection of $X_k$, $S_k$ and $Y_k$ are not specified in line number~\ref{alg:line:xsy}. All our proofs in this paper apply no matter how these are chosen. However, the practical performance of the compression algorithm, which is based on this algorithm, depends on the selection of these matrices. In Section~\ref{sec:expResults}, we compare three options: i) $X_k=S_k=I$, $Y_k = \Sigma(1:\alpha_k; 1:\alpha_k)$ ii) $X_k = Y_k = \Sigma(1:\alpha_k; 1:\alpha_k)^{1/2}$, $S_k = I$ iii) $X_k = Y_k = \Sigma(1:\alpha_k; 1:\alpha_k)$, $S_k = \Sigma(1:\alpha_k; 1:\alpha_k)^{-1} $. In most cases, third option is often the better choice.

This algorithm exposes parallelism in binary tree shape. It achieves maximum $\frac{d}{2}$-level of parallelism at the last level of the tree. Figure~\ref{fig:4dindices} shows that how indices of a $6$-dimensional tensor are divided by this algorithm.

%%\todo[inline]{SK: Draw final diagrams in tikz/xfig}

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.06]{./indices-partition.jpg}
	\end{center}
	\caption{\label{fig:4dindices} Partition of indices for a $6$-dimensional tensor by Algorithm~\ref{alg:tt_parallel}.} 
\end{figure}


\noindent The proof on the bound of TT ranks obtained by Algorithm~\ref{alg:tt_parallel} is described in the following theorem.

\begin{theorem}
	If for each unfolding $A_k$ of a $d$-dimensional tensor \tensor{A}, $rank(A_k)=r_k$, then Algorithm~\ref{alg:tt_parallel} produces a decomposition with TT ranks not higher than $r_k$.
\end{theorem}
\begin{proof}
	Let us consider unfolding matrix $A_k$ of a $d$-dimensional tensor \tensor{A}. The rank of this matrix is $r_k$; hence it can be written as:
	\begin{align*}
	A_k &= U \Sigma V^\intercal\\
	A_k(i_1,i_2,\cdots,i_k;i_{k+1},\cdots, i_d) &= \sum_{\alpha=1}^{r_k} U(i_1,i_2,\cdots,i_k, \alpha)\Sigma(\alpha, \alpha)V(\alpha,i_{k+1},\cdots, i_d)\\
	&= \sum_{\alpha=1}^{r_k} U(i_1,i_2,\cdots,i_k, \alpha)X(\alpha, \alpha)S(\alpha, \alpha)Y(\alpha, \alpha)V(\alpha,i_{k+1},\cdots, i_d)\\
	&= \sum_{\alpha=1}^{r_k} B(i_1,i_2,\cdots,i_k, \alpha)S(\alpha, \alpha)C(\alpha,i_{k+1},\cdots, i_d)\\
	A_k &= BSC^\intercal
	\end{align*}	
%%	\begin{align*}
%%	U &= A_k V(V^\intercal V)^{-1} = A_k X\\
%%	V &= A_k^\intercal U(U^\intercal U)^{-1} = A_k^T W
%%	\end{align*}

	\begin{align*}
%%	\begin{center}
B = A_k (C^\intercal)^{-1}S^{-1} &= A_kZ\\
C = A_k^\intercal (B^\intercal)^{-1}S^{-1} &= A_k^\intercal W
%%	\end{center}
\end{align*}
	
	\noindent Or in the index form,
%%	\begin{align*}
%%	U(i_1,i_2,\cdots, i_k, \alpha) &= \sum_{i_{k+1}=1}^{n_{k+1}}\cdots\sum_{i_d=1}^{n_d} A(i_1, i_2, \cdots, i_d) X(\alpha, i_{k+1},\cdots, i_d)\\
%%	V(\alpha, i_{k+1},\cdots, i_d) &= \sum_{i_1=1}^{n_1} \cdots \sum_{i_k=k}^{n_k} A(i_1, i_2, \cdots, i_d) W(i_1,i_2,\cdots, i_k, \alpha)
%%	\end{align*}
	\begin{align*}
	B(i_1,i_2,\cdots, i_k, \alpha) &= \sum_{i_{k+1}=1}^{n_{k+1}}\cdots\sum_{i_d=1}^{n_d} A(i_1, i_2, \cdots, i_d) Z(\alpha, i_{k+1},\cdots, i_d)\\
	C(\alpha, i_{k+1},\cdots, i_d) &= \sum_{i_1=1}^{n_1} \cdots \sum_{i_k=k}^{n_k} A(i_1, i_2, \cdots, i_d) W(i_1,i_2,\cdots, i_k, \alpha)
	\end{align*}
	
	\noindent $B$ and $C$ can be treated as $k+1$ and $d-k+1$ dimensional tensors respectively. Now we consider unfolding matrices $B_1, B_2, \cdots, B_{k-1}$ and $C_{k+1},\cdots, C_{d-1}$ of $B$ and $C$. We will show that rank($B_{k'}$)$ \le r_{k'}$ $_{1\le k' \le k-1}$ and rank($C_{k'}$)$\le r_{k'}$ $_{k+1\le k' \le d-1}$.
	
	\noindent The rank of $A_{k'}$ is $r_{k'}$. Therefore, it can be represented as,
	\begin{align*}
	A(i_1, i_2, \cdots, i_d) &= \sum_{\beta=1}^{r_{k'}} F(i_1,i_2,\cdots ,i_{k'}, \beta) G(\beta, i_{k'+1},\cdots, i_d)
	\end{align*}
	Now, 
	\begin{align*}
	B_{k'} &= B(i_1,i_2, \cdots, i_{k'}; i_{k'+1},\cdots,i_k, \alpha)\\
	&= \sum_{i_{k+1}=1}^{n_{k+1}}\cdots\sum_{i_d=1}^{n_d} A(i_1, i_2, \cdots, i_d)Z(\alpha, i_{k+1},\cdots, i_d)\\
	&= \sum_{\beta=1}^{r_{k'}}  \sum_{i_{k+1}=1}^{n_{k+1}}\cdots\sum_{i_d=1}^{n_d} F(i_1,i_2,\cdots ,i_{k'}, \beta) G(\beta, i_{k'+1},\cdots, i_d) Z(\alpha, i_{k+1},\cdots, i_d)\\
	&= \sum_{\beta=1}^{r_{k'}} F(i_1,i_2,\cdots ,i_{k'}, \beta) H(\beta,i_{k'+1},\cdots, i_k, \alpha)
	\end{align*}
	\noindent where,
	\begin{align*}
	H(\beta,i_{k'+1},\cdots, i_k, \alpha) =& \sum_{i_{k+1}=1}^{n_{k+1}}\cdots\sum_{i_d=1}^{n_d} G(\beta, i_{k'+1},\cdots, i_d) Z(\alpha, i_{k+1},\cdots, i_d)
	\end{align*}
	\noindent Row and column indices of $B_{k'}$ are now separated. Hence rank($B_{k'}$)$ \le r_{k'}$.
	
	\medskip
	\noindent Similarly for $C_{k'}$,
	\begin{align*}
	C_{k'}&= C(\alpha, i_{k+1}, \cdots, i_{k'}; i_{k'+1},\cdots, i_d)\\
	&= \sum_{i_1=1}^{n_1} \cdots \sum_{i_k=k}^{n_k} A(i_1, i_2, \cdots, i_d) W(i_1,i_2,\cdots, i_k, \alpha)\\
	&= \sum_{\beta=1}^{r_{k'}} \sum_{i_1=1}^{n_1} \cdots \sum_{i_k=k}^{n_k} F(i_1,i_2,\cdots ,i_{k'}, \beta) G(\beta, i_{k'+1},\cdots, i_d) W(i_1,i_2,\cdots, i_k, \alpha)\\ 
	&= \sum_{\beta=1}^{r_{k'}} M(\alpha, i_{k+1}, \cdots, i_{k'}, \beta) G(\beta, i_{k'+1},\cdots, i_d)
	\end{align*}	
	\noindent where
	\begin{align*}
	M(\alpha, i_{k+1}, \cdots, i_{k'}, \beta) &= \sum_{i_1=1}^{n_1} \cdots \sum_{i_k=k}^{n_k} F(i_1,i_2,\cdots ,i_{k'}, \beta) W(i_1,i_2,\cdots, i_k, \alpha)
	\end{align*}
	\noindent Here also row and column indices of $C_{k'}$ are separated. Hence rank($C_{k'}$)$ \le r_{k'}$.
	
	%%	This process continues recursively until each subtensor has exactly one external index. 
	When Algorithm~\ref{alg:tt_parallel} terminates, we have a list of tensors with exactly one external index. These tensors are cores of a TT-representation. As the above proof holds for each recursive partition, hence TT ranks of the decomposition produced by Algorithm~\ref{alg:tt_parallel} are bounded by $r_k$. This completes the proof.
\end{proof}


\section{Tensor Train Compression and Our Heuristics}
\label{sec:heuristics}
% 1 page
%%\begin{itemize}
%%	\item Our heuristics
%%	\item different performance metrics
%%\end{itemize}

%%Tensor decompositions are often approximated with a controllable global error to cope with noise and numerical instability. In this section, we propose different heuristics such that the approximated tensor is close to the original tensor by some prescribed accuracy.

Tensor decompositions are often approximated to cope with noise and numerical instability. In practical computations, rank-$r_k$ approximation of a tensor or an approximated tensor with certain accuracy  is desired. Algorithm~\ref{alg:tt_parallel} can be used to compute rank-$r_k$ approximation of a tensor instead of exact low-rank decomposition. However we are yet to claim any optimality on its approximation and it is a part of our future work.

We modify Algorithm~\ref{alg:tt_parallel} to compute an approximated tensor in TT-format which is close by the prescribed accuracy, and present it in Algorithm~\ref{alg:tt_parallel_approx}.
 
\begin{algorithm}[htb]
	\caption{\label{alg:tt_parallel_approx}TT-parallel-approx (parallel Tensor Train approximation)}
	\begin{algorithmic}[1]
		\REQUIRE $d$-dimensional tensor \tensor{A} and expected accuracy $\epsilon$ 
		\ENSURE Cores $G_k(\alpha_{k-1}, n_k, \alpha_k) _{1\le k\le d}$ of the approximated tensor $B$ in TT-format such that $||A-B||_F$ is close to or less than $\epsilon$
		\IF{$nE(\tensor{A})> $$1$} 
		\STATE Find the middle external index $k$
		\STATE Compute unfolding matrix $A_k$ 
		\STATE Compute SVD: $A_k = U \Sigma V^\intercal$
		\STATE Compute truncation\_accuracy $\delta$
		\STATE Compute $\alpha_k$ such that $A_k = U(;1:\alpha_k) \Sigma(1:\alpha_k; 1:\alpha_k) V^\intercal(1:\alpha_k;) + E_k$ and $||E_K||_F \le \delta$
		\STATE Select diagonal matrices $X_k$, $S_k$ and $Y_k$ such that $X_kS_kY_k = \Sigma(1:\alpha_k; 1:\alpha_k)$
		\STATE $\tensor{A}$$_{left}$ = Tensor($U(;1:\alpha_k)X_k$)
		\STATE list1 = TT-parallel-approx($\tensor{A}$$_{left}$, $\epsilon_1$)
		\STATE $\tensor{A}$$_{right}$ = Tensor($Y_kV^\intercal(1:\alpha_k;)$)
		\STATE list2 = TT-parallel-approx($\tensor{A}$$_{right}$, $\epsilon_2$)
		\RETURN \{list1, list2\}
		\ELSE 
		\STATE Find the external index $k$
		\IF {$k$ is the last index of \tensor{A}} 
		\STATE $\alpha_d = 1$
		\STATE $G_k = \tensor{A}$
		\RETURN $G_k$
		\ENDIF
		\IF {$k$ is the first index of \tensor{A}}
		\STATE $\alpha_0 = 1$
		\ENDIF
		\STATE Compute unfolding matrix $A_k$
		\STATE Update $A_k$, $A_k = A_kS_k$
		\STATE  $G_k$ = Tensor($A_k$)
		\STATE return $G_k$
		\ENDIF
	\end{algorithmic}
\end{algorithm}

Values of $\delta$, $\epsilon_1$ and $\epsilon_2$ in Algorithm~\ref{alg:tt_parallel_approx} depend on the selection of diagonal matrices $X_k$, $Y_k$ and $S_K$. We show in next subsection that how product of approximated matrices impact the accuracy of the result when applied with svd truncation. Based on the expression of next subsection, we compute $\epsilon_1$ and $\epsilon_2$  for different choices of $X_k$, $Y_k$ and $S_K$ in Section~\ref{sec:heuristics:all}.  


%%we propose different heuristics such that the approximated tensor is close to the original tensor by some prescribed accuracy. 
\subsection{Frobenius Error with Product of Approximated Matrices}
\label{sec:heuristics:approxproduct}

The following equation represents the singular value decomposition of a real matrix $A$.
\begin{equation*}
A=U\Sigma V^T
\end{equation*}

\noindent We can also write the above equation in the following way.
\begin{align*}
A&=(U_1 U_2)\begin{pmatrix}
\Sigma_1 & 0\\
0 & \Sigma_2
\end{pmatrix}(V_1 V_2)^T\\
&= U_1\Sigma_1 V_1^T + U_2 \Sigma_2 V_2^T\\
&= U_1\Sigma_1 V_1^T + E_A \\
&= BSC + E_A
\end{align*}

\noindent Here $B = U_1 X$, $C=YV_1^T$ and $XSY = \Sigma_1$.  Matrices $B$ and $C$ are approximated to $\hat{B}$ and $\hat{C}$, i.e., $B = \hat{B} + E_B$ and $C = \hat{C} + E_C$. $X$, $Y$ and $S$ are diagonal matrices. $E_A$, $E_B$ and $E_C$ represent error matrices corresponding to low-rank approximation of $A$, $B$ and $C$.

\noindent Our goal is to find an expression for $||A - \hat{B} S \hat{C}||_F$ in terms of $||E_A||_F$, $||E_B||_F$ and $||E_C||_F$.

\begin{align*}
A - \hat{B} S \hat{C} &= A - (B-E_B)S(C-E_C)\\
&= A - BSC + BSE_C +  E_BSC - E_BSE_C\\
&= E_A + BSE_C +  E_BSC - E_BSE_C
\end{align*}
Now we take square of Frobenius norm on both sides,
\begin{align*}
||A - \hat{B} S \hat{C}||_F^2 =& ||E_A + BSE_C +  E_BSC - E_BSE_C||_F^2 \\
=& ||E_A||_F^2 + ||BSE_C||_F^2 + ||E_BSC||_F^2 + ||E_BSE_C||_F^2 \\
& + 2\boldsymbol{<}E_A, BSE_C\boldsymbol{>}_F + 2<E_A, E_BSC>_F -2 <E_A, E_BSE_C>_F\\
& + 2 <BSE_C, E_BSC>_F -2 <BSE_C, E_BSE_C>_F - 2<E_BSC, E_BSE_C>_F
\end{align*}

\noindent Here $<P, Q>_F$ denotes frobenius inner product of matrices P and Q, and it is defined as: $<P, Q>_F = trace(P^TQ) = trace(PQ^T)$.

\noindent As $U$ and $V$ are othogonal matrices in singular value decomposition of $A$. Hence we obtain the following expressions:
\begin{align*}
U_1^TU_2 = 0 & \implies B^TE_A = 0 \implies <E_A, BSE_C>_F = 0\\
V_1^TV_2 = 0 & \implies CE_A^T = 0 \implies <E_A, E_BSC>_F = 0  
\end{align*}
After putting these values in $||A - \hat{B}S\hat{C}||_F^2$ expression, we get
\begin{align*}
||A - \hat{B} S \hat{C}||_F^2 =& ||E_A||_F^2 + ||BSE_C||_F^2 + ||E_BSC||_F^2 + ||E_BSE_C||_F^2 \\
& -2 <E_A, E_BSE_C>_F  + 2 <BSE_C, E_BSC>_F\\
& -2 <BSE_C, E_BSE_C>_F - 2<E_BSC, E_BSE_C>_F
\end{align*}

\noindent In general, targeted error of any compression is very low, therefore we assume that any term involving more than one error matrix would be close to zero. With this assumption, the above equation can be written as

\begin{equation*}
%%\label{eq:approx}
||A - \hat{B} S \hat{C}||_F^2 \approx ||E_A||_F^2 + ||BSE_C||_F^2 + ||E_BSC||_F^2
\end{equation*}

\subsection{Different Heuristics}
\label{sec:heuristics:all}

It is immediate that the expression of the previous subsection can be applied for each unfolding of Algorithm~\ref{alg:tt_parallel_approx}. Similar to the approach of~\cite{tt}, we take $||E_{A_k}||_F \le \frac{\epsilon}{\sqrt{d-1}}$. Let $B$ and $C$ correspond to unfolding of $d_1$ and $d_2$ dimensional tensors and $d_1 + d_2 = d$. For simplicity, we  assume $\frac{||E_B||_F^2}{d_1-1} = \frac{||E_C||_F^2}{d_2-1} = \delta^2$. This implies $\frac{\epsilon_1^2}{d_1-1} = \frac{\epsilon_2^2}{d_2-1} = \delta^2$.

\noindent Now we propose three heuristics such that overall error of approximation of Alogrithm~\ref{alg:tt_parallel_approx} is close or less to $\epsilon$.
\begin{enumerate}
	\item \textit{H1}: Here we select $X = I$, $Y = \Sigma_1$ and $S = I$.	
	\begin{align*}
	||A - \hat{B} S \hat{C}||_F^2 &\approx ||E_A||_F^2 + ||U_1IIE_C||_F^2 + ||E_BI\Sigma_1V_1^T||_F^2 \\
	&\le \frac{\epsilon^2}{d-1}  + ||E_C||_F^2 + ||E_B\Sigma_1||_F^2 \\
	\text{From Cauchy} & \text{-Schwarz inequality,}\\
	&\le \frac{\epsilon^2}{d-1}  + ||E_C||_F^2 + ||E_B||_F^2  ||\Sigma_1||_F^2  \\
	&= \frac{\epsilon^2}{d-1} + \delta^2 (d_2-1)+ \delta^2  (d_1-1) trace(\Sigma_1^2)\le \epsilon^2.
	\end{align*}
	\noindent After simplifying this expression we obtain
	\begin{align*}
	\delta &\le \epsilon \sqrt{\frac{d-2}{(d-1) (d_2 -1 + (d_1-1) trace(\Sigma_1^2))}}.
	\end{align*}
	
	\item \textit{H2}: We choose $X=Y=\Sigma_1^{\frac{1}{2}}$ and $S=I$.
	
	\begin{align*}
	||A - \hat{B} S \hat{C}||_F^2 &\approx ||E_A||_F^2 + ||U_1 \Sigma_1^{\frac{1}{2}} IE_C||_F^2 + ||E_B I \Sigma_1^{\frac{1}{2}} V_1^T||_F^2 \\
	&\le \frac{\epsilon^2}{d-1}  + ||\Sigma_1^{\frac{1}{2}} E_C||_F^2 + ||E_B \Sigma_1^{\frac{1}{2}}||_F^2 \\
	\text{From Cauchy} & \text{-Schwarz inequality,}\\
	&\le \frac{\epsilon^2}{d-1}  + ||E_C||_F^2 ||\Sigma_1^{\frac{1}{2}}||_F^2 + ||E_B||_F^2  ||\Sigma_1^{\frac{1}{2}}||_F^2  \\
	&= \frac{\epsilon^2}{d-1} + \delta^2 (d_2-1) trace(\Sigma_1) + \delta^2 (d_1 -1) trace(\Sigma_1)\le \epsilon^2.
	\end{align*} 
	\noindent After simplifying this expression we obtain
	\begin{align*}
	\delta &\le \frac{\epsilon}{\sqrt{(d-1)trace(\Sigma_1)}}.
	\end{align*}
	
	\item \textit{H3}: We choose $X=Y=\Sigma_1$ and $S=\Sigma_1^{-1}$.
	\begin{align*}
	||A - \hat{B} S \hat{C}||_F^2 &\approx ||E_A||_F^2 + ||U_1\Sigma_1 \Sigma_1^{-1} E_C||_F^2 + ||E_B \Sigma_1^{-1} \Sigma_1 V_1^T||_F^2\\
	&\le  \frac{\epsilon^2}{d-1} + ||E_C||_F^2 +  ||E_B||_F^2 \\
	&= \frac{\epsilon^2}{d-1} + \delta^2 (d_2 -1) + \delta^2 (d_1-1) \le \epsilon^2
	\end{align*}
	\noindent After simplifying this expression we get
	\begin{align*}
	\delta &\le \frac{\epsilon}{\sqrt{d-1}}.
	\end{align*}	
\end{enumerate}

The following table summarizes all parameters of different heuristics for Algorithm~\ref{alg:tt_parallel_approx}. Let left and right subtensors are denoted by \tensor{B} and \tensor{C}, and have $d_1$ and $d_2$ external dimensions.

\begin{table}[htb]
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Heuristic & Description & $\epsilon_{A_k}$ & $\epsilon_1$ & $\epsilon_2$\\ \hline
	H1 &  $X = I$, $Y = \Sigma_1$, $S = I$ & $\frac{\epsilon}{\sqrt{d-1}}$ & $\epsilon \sqrt{\frac{(d-2)(d_1-1)}{(d-1) (d_2 -1 + (d_1-1) trace(\Sigma_1^2))}}$ & $\epsilon \sqrt{\frac{(d-2)(d_2-1)}{(d-1) (d_2 -1 + (d_1-1) trace(\Sigma_1^2))}}$\\ \hline
	H2 & $X=Y=\Sigma_1^{\frac{1}{2}}$, $S=I$ & $\frac{\epsilon}{\sqrt{d-1}}$ &
	$\epsilon\sqrt{\frac{d_1-1}{(d-1)trace(\Sigma_1)}}$ & $\epsilon\sqrt{\frac{d_2-1}{(d-1)trace(\Sigma_1)}}$\\ \hline
	H3 & $X=Y=\Sigma_1$, $S=\Sigma_1^{-1}$ & $\frac{\epsilon}{\sqrt{d-1}}$ &
	$\epsilon\sqrt{\frac{d_1-1}{d-1}}$ & $\epsilon\sqrt{\frac{d_2-1}{d-1}}$\\ \hline 
\end{tabular}
\caption{Details of all considered heuristics\label{tab:heuristics}.}
\end{table}
It is not hard to observe that svd truncation of $\frac{\epsilon}{\sqrt{d-1}}$ is used for each unfolding in \textit{H3}.

%%
%%The following equation represents the singular value decomposition of a real matrix $A$.
%%\begin{equation*}
%%A=U\Sigma V^T
%%\end{equation*}
%%
%%\noindent We can also write the above equation in the following way.
%%\begin{align*}
%%A&=(U_1 U_2)\begin{pmatrix}
%%\Sigma_1 & 0\\
%%0 & \Sigma_2
%%\end{pmatrix}(V_1 V_2)^T\\
%%&= U_1\Sigma_1 V_1^T + U_2 \Sigma_2 V_2^T\\
%%&= U_1\Sigma_1 V_1^T + E_A \\
%%&= BSC + E_A
%%\end{align*}
%%
%%\noindent Here $B = U_1 X$, $C=YV_1^T$ and $XSY = \Sigma_1$.  Matrices $B$ and $C$ are approximated to $\hat{B}$ and $\hat{C}$, i.e., $B = \hat{B} + E_B$ and $C = \hat{C} + E_C$.

%%\medskip
%%{\color{blue}
%%	\noindent Our goal is to choose $X$, $Y$ and $S$ such that $||A - \hat{B} S \hat{C}||_F$ can be bounded in terms of $||E_A||_F$, $||E_B||_F$ and $||E_C||_F$
%%}.

%%\begin{align*}
%%A - \hat{B} S \hat{C} &= A - (B-E_B)S(C-E_C)\\
%%&= A - BSC + BSE_C +  E_BSC - E_BSE_C\\
%%&= E_A + BSE_C +  E_BSC - E_BSE_C
%%\end{align*}
%%Now we take square of Frobenius norm on both sides,
%%\begin{align*}
%%||A - \hat{B} S \hat{C}||_F^2 =& ||E_A + BSE_C +  E_BSC - E_BSE_C||_F^2 \\
%%=& ||E_A||_F^2 + ||BSE_C||_F^2 + ||E_BSC||_F^2 + ||E_BSE_C||_F^2 \\
%%& + 2\boldsymbol{<}E_A, BSE_C\boldsymbol{>}_F + 2<E_A, E_BSC>_F -2 <E_A, E_BSE_C>_F\\
%%& + 2 <BSE_C, E_BSC>_F -2 <BSE_C, E_BSE_C>_F - 2<E_BSC, E_BSE_C>_F
%%\end{align*}
%%
%%\noindent Here $<P, Q>_F$ denotes frobenius inner product of matrices P and Q, and it is defined as: $<P, Q>_F = trace(P^TQ) = trace(PQ^T)$.
%%
%%\noindent As $U$ and $V$ are othogonal matrices in singular value decomposition of $A$. Hence we obtain the following expressions:
%%\begin{align*}
%%U_1^TU_2 = 0 & \implies B^TE_A = 0 \implies <E_A, BSE_C>_F = 0\\
%%V_1^TV_2 = 0 & \implies CE_A^T = 0 \implies <E_A, E_BSC>_F = 0  
%%\end{align*}
%%After putting these values in $||A - \hat{B}S\hat{C}||_F^2$ expression, we get
%%\begin{align*}
%%||A - \hat{B} S \hat{C}||_F^2 =& ||E_A||_F^2 + ||BSE_C||_F^2 + ||E_BSC||_F^2 + ||E_BSE_C||_F^2 \\
%%& -2 <E_A, E_BSE_C>_F  + 2 <BSE_C, E_BSC>_F\\
%%& -2 <BSE_C, E_BSE_C>_F - 2<E_BSC, E_BSE_C>_F
%%\end{align*}
%%
%%\noindent In general, expected error of any compression is very low, therefore we assume that any term involving more than one error matrix would be close to zero. With this assumption, the above equation can be written as
%%
%%\begin{equation}\label{eq:approx}
%%||A - \hat{B} S \hat{C}||_F^2 \approx ||E_A||_F^2 + ||BSE_C||_F^2 + ||E_BSC||_F^2
%%\end{equation}

%%\todo[inline]{SK: Make smooth transition from matrices to tensors. Present matrix results as a Lemma and use it for tensors.}
%%\todo[inline]{SK: Definitions of B and C are not clear in unfolding of tensors.}

%%\noindent Similar to the approach of ~\cite{.}, we take $||E_A||_F \le \frac{\epsilon}{\sqrt{d-1}}$. Let $B$ and $C$ correspond to unfolding of $d_1$ and $d_2$ dimensional tensors and $d_1 + d_2 = d$. For simplicity, we  assume $\frac{||E_B||_F^2}{d_1-1} = \frac{||E_C||_F^2}{d_2-1} = \delta^2$.
%%
%%\noindent Now we propose three heuristics such that Eq ~\ref{eq:approx} is bounded by $\epsilon$.
%%\todo[inline]{SK: find catchy names for heuristics.}
%%\todo[inline]{SK: mention how cauchy schwarz inequality is applied.}
%%\begin{itemize}
%%	\item Heuristic1: Here we select $X = I$, $Y = \Sigma_1$ and $S = I$.	
%%	\begin{align*}
%%	||A - \hat{B} S \hat{C}||_F^2 &\approx ||E_A||_F^2 + ||U_1IIE_C||_F^2 + ||E_BI\Sigma_1V_1^T||_F^2 \\
%%	&\le \frac{\epsilon^2}{d-1}  + ||E_C||_F^2 + ||E_B\Sigma_1||_F^2 \\
%%	\text{From Cauchy} & \text{-Schwarz inequality,}\\
%%	&\le \frac{\epsilon^2}{d-1}  + ||E_C||_F^2 + ||E_B||_F^2  ||\Sigma_1||_F^2  \\
%%	&= \frac{\epsilon^2}{d-1} + \delta^2 (d_2-1)+ \delta^2  (d_1-1) trace(\Sigma_1^2)\le \epsilon^2.
%%	\end{align*}
%%	\noindent After simplifying this expression we obtain
%%	%%	
%%	%%	\noindent $||A - \hat{B}\hat{C}||_F^2 \approx \frac{\epsilon^2}{d-1} + ||U_1IE_C||_F^2 + ||E_B\Sigma_1V_1||_F^2  \le \frac{\epsilon^2}{d-1} + \delta^2 + \delta^2 trace(\Sigma_1^2)\le \epsilon^2$. After simplifying this expression we obtain
%%	\begin{align*}
%%	\delta &\le \epsilon \sqrt{\frac{d-2}{(d-1) (d_2 -1 + (d_1-1) trace(\Sigma_1^2))}}.
%%	\end{align*}
%%	
%%	\item Heuristic2: We choose $X=Y=\Sigma_1^{\frac{1}{2}}$ and $S=I$.
%%	
%%	\begin{align*}
%%	||A - \hat{B} S \hat{C}||_F^2 &\approx ||E_A||_F^2 + ||U_1 \Sigma_1^{\frac{1}{2}} IE_C||_F^2 + ||E_B I \Sigma_1^{\frac{1}{2}} V_1^T||_F^2 \\
%%	&\le \frac{\epsilon^2}{d-1}  + ||\Sigma_1^{\frac{1}{2}} E_C||_F^2 + ||E_B \Sigma_1^{\frac{1}{2}}||_F^2 \\
%%	\text{From Cauchy} & \text{-Schwarz inequality,}\\
%%	&\le \frac{\epsilon^2}{d-1}  + ||E_C||_F^2 ||\Sigma_1^{\frac{1}{2}}||_F^2 + ||E_B||_F^2  ||\Sigma_1^{\frac{1}{2}}||_F^2  \\
%%	&= \frac{\epsilon^2}{d-1} + \delta^2 (d_2-1) trace(\Sigma_1) + \delta^2 (d_1 -1) trace(\Sigma_1)\le \epsilon^2.
%%	\end{align*}
%%	
%%	%%	\noindent $||A - \hat{B}\hat{C}||_F^2 \approx \frac{\epsilon^2}{d-1} + ||U_1 \Sigma_1^{\frac{1}{2}} E_C||_F^2 + ||E_B \Sigma_1^{\frac{1}{2}} V_1||_F^2  \le \frac{\epsilon^2}{d-1} + \delta^2 trace(\Sigma_1) + \delta^2 trace(\Sigma_1)\le \epsilon^2$. 
%%	
%%	\noindent After simplifying this expression we obtain
%%	\begin{align*}
%%	\delta &\le \frac{\epsilon}{\sqrt{(d-1)trace(\Sigma_1)}}.
%%	\end{align*}
%%	
%%	\item Heuristic3: We choose $X=Y=\Sigma_1$ and $S=\Sigma_1^{-1}$.
%%	\begin{align*}
%%	||A - \hat{B} S \hat{C}||_F^2 &\approx ||E_A||_F^2 + ||U_1\Sigma_1 \Sigma_1^{-1} E_C||_F^2 + ||E_B \Sigma_1^{-1} \Sigma_1 V_1^T||_F^2\\
%%	&\le  \frac{\epsilon^2}{d-1} + ||E_C||_F^2 +  ||E_B||_F^2 \\
%%	&= \frac{\epsilon^2}{d-1} + \delta^2 (d_2 -1) + \delta^2 (d_1-1) \le \epsilon^2
%%	\end{align*}
%%	\noindent After simplifying this expression we get
%%	\begin{align*}
%%	\delta &\le \frac{\epsilon}{\sqrt{d-1}}.
%%	\end{align*}	
%%\end{itemize}


%%
%%\noindent \fbox{ 
%%	\begin{minipage}{0.95\linewidth}
%%		\textit{Oseledets} selected $ X = I$, $S=I$ and $B = \hat{B} = U_1$, i.e., $E_B = 0$. It allows him to prove that $||A - \hat{B} S \hat{C}||_F^2$ = $||A - U_1\hat{C}||_F^2$ $\le$ $||E_A||_F^2$ + $||E_C||_F^2$.
%%	\end{minipage}
%%	
%%}

%%\newpage
%%
%%\section{17th Sept Discussion}
%%\begin{figure*}[htb]
%%	\includegraphics[scale=0.1]{./diagrams/pttHeuristic3.jpg}
%%	%%	\caption{discussion on 17th Sept.}
%%\end{figure*}

%%\newpage

\section{Experimental Results}
\label{sec:expResults}
\begin{itemize}
	\item Low rank functions and their descriptions
	\item Compression ratio tables
	\item plot ratio of actual/min storage (may be with ratio)
	\item Tensor arising in Molecular chemistry
\end{itemize}


\noindent We evaluate our heuristics for the following low rank functions.
\begin{table}[htb]
	\centering
	\begin{tabular}{|l|c|}
		\hline
		Log & $\log(\sum_{j=1}^{N}j i_j)$\\ \hline
		Sin & $\sin(\sum_{j=1}^{N}i_j)$\\ \hline
		Square-root & $\frac{1}{\sqrt{\sum_{j=1}^{N}i_j^2}}$\\ \hline
		Cube-root & $\frac{1}{\sqrt[3]{\sum_{j=1}^{N}i_j^3}}$\\ \hline
		Penta-root& $\frac{1}{\sqrt[5]{\sum_{j=1}^{N}i_j^5}}$\\ \hline
	\end{tabular}
	\caption{Low rank functions.\label{tab:lowRankFunctions}}
\end{table}

\noindent First we consider $N=12$ and $i_j \in \{1, 2, 3, 4\}$. This configuration produces a $12$-dimensional tensor with $4^{12}$ elements for each low rank function. Tables~\ref{tab:12-dim-10-3} and ~\ref{tab:12-dim-10-6} assess efficacy of all the proposed heuristics  for this setting.

Next we take $N=6$ and $i_j \in \{1,2 \cdots ,16\}$. This setting creates a $6$-dimensional tensor with $16^6$ elements for each low rank function of Table~\ref{tab:lowRankFunctions}. Tables~\ref{tab:6-dim-10-3} and ~\ref{tab:6-dim-10-6} compare performance of all the considered heuristics for this configuration.


\begin{table}[!htb]
	\centering\tiny
	\rotatebox{0}{
		%%\begin{tabular}{*{13}{|c}|}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
			%%		\toprule
			\hline
			\multicolumn{13}{|c|}{12-dimensional tensors with $4^{12}$ elements}\\ \hline
			\multicolumn{13}{|c|}{Prescribed accuracy = $10^{-3}$}\\ \hline
			\multirow{3}{*}{Function} &  \multicolumn{2}{|c|}{\multirow{2}{*}{TT}} & \multicolumn{4}{|c|}{H1}& \multicolumn{4}{|c|}{H2} & \multicolumn{2}{|c|}{\multirow{2}{*}{H3}}\\ \cline{4-11}
			&\multicolumn{2}{|c|}{} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{} \\ \cline{2-13}
			{} & Com(\%) & error & Com(\%) & error 
			& Com(\%) & error & Com(\%) & error
			& Com(\%) & error & Com(\%) & error\\ \hline
			Log & 99.996 & 3.6922e-04
			& 99.995 & 0.27170 & 99.987 & 0.0018235 
			& 99.994 & 7.1162e-05 & 99.989 & 7.0418e-05 
			& 99.996 & 3.6922e-04 \\ \hline 
			Sin & 99.999 & 2.6151e-09
			& 99.999 & 1.3933e-11 & 99.999 & 1.3933e-11
			& 99.999 & 6.1444e-12 & 99.999 & 6.1444e-12
			& 99.999 & 1.2521e-11 \\ \hline
			Square & 99.994 & 3.2240e-04 
			& 99.989 & 0.0015409 & 99.985 & 2.2441e-04 
			& 99.990 & 2.0407e-04 & 99.986 & 2.0365e-04
			& 99.994 & 3.2234e-04\\ \hline
			Cube & 99.991 & 4.2331e-04
			& 99.982 & 8.7550e-04 & 99.976 & 1.6722e-04 
			& 99.984 & 6.2147e-05 & 99.977 & 6.0975e-05
			& 99.991 & 5.0419e-04\\ \hline
			Penta & 99.986 & 4.7738e-04
			& 99.976 & 0.0046476 & 99.964 & 3.7309e-04 
			& 99.977 & 2.9834e-04 & 99.968 & 2.9810e-04
			& 99.986 & 4.7735e-04\\ \hline
			%%		{}   & EF   & CHF    & EF2   & CHF2\\
			%%		1   &  17.5 & 19.1   & 17.5  & 19.1\\
			%%		2   &  11.8 & 12.7   & 29.3  & 31.9\\
			%%		3   &  6.6  &  5.6   & 35.9  & 37.4\\
			%%		\bottomrule
		\end{tabular}
	}
	\caption{Comparison of different heuristics for $12$-dimensional tensors with  prescribed accuracy of $10^{-3}$.\label{tab:12-dim-10-3}}
\end{table}



\begin{table}[!htb]
	\centering\tiny
	\rotatebox{0}{
		%%\begin{tabular}{*{13}{|c}|}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
			%%		\toprule
			\hline
			\multicolumn{13}{|c|}{12-dimensional tensors with $4^{12}$ elements}\\ \hline
			\multicolumn{13}{|c|}{Prescribed accuracy = $10^{-6}$}\\ \hline
			\multirow{3}{*}{Function} &  \multicolumn{2}{|c|}{\multirow{2}{*}{TT}} & \multicolumn{4}{|c|}{H1}& \multicolumn{4}{|c|}{H2} & \multicolumn{2}{|c|}{\multirow{2}{*}{H3}}\\ \cline{4-11}
			&\multicolumn{2}{|c|}{} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{} \\ \cline{2-13}
			{} & Com(\%) & error & Com(\%) & error 
			& Com(\%) & error & Com(\%) & error
			& Com(\%) & error & Com(\%) & error\\ \hline
			Log & 99.993 & 2.2710e-07 & 99.965 & 7.0427e-05 & 99.939 & 8.9588e-07 & 99.983 & 2.9114e-08 & 99.799 & 2.8198e-08 & 99.993 & 2.2649e-07 \\ \hline
			Sin & 99.999 & 2.6151e-09 & 99.999 & 1.3933e-11 & 99.999 & 1.3933e-11 & 99.999 & 6.1444e-12 & 99.999 & 6.1444e-12 & 99.999 & 1.2521e-11 \\ \hline
			Square & 99.987 & 1.8341e-07 & 99.970 & 1.4783e-06 & 99.944 & 1.4041e-07 & 99.973 & 1.1180e-07 & 99.952 & 1.1180e-07 & 99.987 & 1.8341e-07 \\ \hline
			Cube & 99.981 & 4.8845e-07 & 99.953 & 5.7114e-07 & 99.915 & 9.2578e-08 & 99.961 & 8.5180e-08 & 99.912 & 8.5179e-08 & 99.981 & 4.8845e-07 \\ \hline
			Penta & 99.971 & 4.8356e-07 & 99.930 & 1.0562e-05 & 99.873 & 1.2493e-07 & 99.946 & 5.6684e-08 & 99.870 & 5.6642e-08 & 99.970 & 3.9988e-07 \\ \hline
			%%		{}   & EF   & CHF    & EF2   & CHF2\\
			%%		1   &  17.5 & 19.1   & 17.5  & 19.1\\
			%%		2   &  11.8 & 12.7   & 29.3  & 31.9\\
			%%		3   &  6.6  &  5.6   & 35.9  & 37.4\\
			%%		\bottomrule
		\end{tabular}
	}
	\caption{Comparison of different heuristics for $12$-dimensional tensors with  prescribed accuracy of $10^{-6}$.\label{tab:12-dim-10-6}}
\end{table}



%%16^6 tensor results
\begin{table}[!htb]
	\centering\tiny
	\rotatebox{0}{
		%%\begin{tabular}{*{13}{|c}|}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
			%%		\toprule
			\hline
			\multicolumn{13}{|c|}{12-dimensional tensors with $16^{6}$ elements}\\ \hline
			\multicolumn{13}{|c|}{Prescribed accuracy = $10^{-3}$}\\ \hline
			\multirow{3}{*}{Function} &  \multicolumn{2}{|c|}{\multirow{2}{*}{TT}} & \multicolumn{4}{|c|}{H1}& \multicolumn{4}{|c|}{H2} & \multicolumn{2}{|c|}{\multirow{2}{*}{H3}}\\ \cline{4-11}
			&\multicolumn{2}{|c|}{} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{} \\ \cline{2-13}
			{} & Com(\%) & error & Com(\%) & error 
			& Com(\%) & error & Com(\%) & error
			& Com(\%) & error & Com(\%) & error\\ \hline
			Log & 99.987 & 2.8313e-04 & 99.981 & 0.074940 & 99.953 & 7.5782e-05 & 99.983 & 2.5667e-04 & 99.973 & 7.2675e-05 & 99.987 & 2.8315e-04 \\ \hline 
			Sin & 99.998 & 4.2061e-10 & 99.998 & 3.4096e-12 & 99.998 & 3.4096e-12 & 99.998 & 3.4856e-12 & 99.998 & 3.4856e-12 & 99.998 & 4.4089e-12 \\ \hline 
			Square & 99.976 & 4.5874e-04 & 99.960 & 8.8471e-04 & 99.948 & 2.0805e-04 & 99.965 & 2.0653e-04 & 99.952 & 2.0326e-04 & 99.976 & 4.5881e-04 \\ \hline 
			Cube & 99.962 & 6.1491e-04 & 99.939 & 9.5360e-04 & 99.920 & 2.9057e-04 & 99.945 & 2.8963e-04 & 99.928 & 2.8822e-04 & 99.962 & 6.1498e-04 \\ \hline 
			Penta & 99.928 & 4.4294e-04 & 99.893 & 5.6615e-04 & 99.867 & 2.2684e-04 & 99.905 & 2.2351e-04 & 99.879 & 2.2176e-04 & 99.928 & 4.4298e-04 \\ \hline
			%%		{}   & EF   & CHF    & EF2   & CHF2\\
			%%		1   &  17.5 & 19.1   & 17.5  & 19.1\\
			%%		2   &  11.8 & 12.7   & 29.3  & 31.9\\
			%%		3   &  6.6  &  5.6   & 35.9  & 37.4\\
			%%		\bottomrule
		\end{tabular}
	}
	\caption{Comparison of different heuristics for $6$-dimensional tensors with prescribed accuracy of $10^{-3}$.\label{tab:6-dim-10-3}}
\end{table}



\begin{table}[!htb]
	\centering\tiny
	\rotatebox{0}{
		%%\begin{tabular}{*{13}{|c}|}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
			%%		\toprule
			\hline
			\multicolumn{13}{|c|}{12-dimensional tensors with $16^{6}$ elements}\\ \hline
			\multicolumn{13}{|c|}{Prescribed accuracy = $10^{-6}$}\\ \hline
			\multirow{3}{*}{Function} &  \multicolumn{2}{|c|}{\multirow{2}{*}{TT}} & \multicolumn{4}{|c|}{H1}& \multicolumn{4}{|c|}{H2} & \multicolumn{2}{|c|}{\multirow{2}{*}{H3}}\\ \cline{4-11}
			&\multicolumn{2}{|c|}{} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{} \\ \cline{2-13}
			{} & Com(\%) & error & Com(\%) & error 
			& Com(\%) & error & Com(\%) & error
			& Com(\%) & error & Com(\%) & error\\ \hline
			Log & 99.974 & 4.1557e-07 & 99.914 & 9.6639e-05 & 99.740 & 1.1468e-06 & 99.948 & 3.5888e-07 & 99.824 & 3.5837e-07 & 99.974 & 4.1556e-07 \\ \hline 
			Sin & 99.998 & 4.2061e-10 & 99.998 & 3.4096e-12 & 99.998 & 3.4096e-12 & 99.998 & 3.4856e-12 & 99.998 & 3.4856e-12 & 99.998 & 4.4089e-12 \\ \hline 
			Square & 99.945 & 3.9903e-07 & 99.907 & 3.3444e-07 & 99.841 & 2.8413e-07 & 99.918 & 2.8218e-07 & 99.902 & 2.8217e-07 & 99.945 & 3.9899e-07 \\ \hline 
			Cube & 99.915 & 6.9835e-07 & 99.860 & 4.1924e-07 & 99.783 & 1.6103e-07 & 99.869 & 1.5221e-07 & 99.854 & 1.5221e-07 & 99.915 & 6.9848e-07 \\ \hline 
			Penta & 99.845 & 4.5363e-07 & 99.772 & 1.0574e-06 & 99.696 & 1.9581e-07 & 99.794 & 1.8025e-07 & 99.772 & 1.8025e-07 & 99.845 & 4.5380e-07 \\ \hline
			%%		{}   & EF   & CHF    & EF2   & CHF2\\
			%%		1   &  17.5 & 19.1   & 17.5  & 19.1\\
			%%		2   &  11.8 & 12.7   & 29.3  & 31.9\\
			%%		3   &  6.6  &  5.6   & 35.9  & 37.4\\
			%%		\bottomrule
		\end{tabular}
	}
	\caption{Comparison of different heuristics for $6$-dimensional tensors with prescribed accuracy of $10^{-6}$.\label{tab:6-dim-10-6}}
\end{table}



%% Chemistry results
Tables~\ref{tab:chem-10-3} and ~\ref{tab:chem-10-6} show comparison of different heuristics for 4-dimensional tensors obtained by running molecular simulations.

\begin{table}[!htb]
	\centering\tiny
	\rotatebox{0}{
		%%		\begin{minipage}{\textheight}
		%%\begin{tabular}{*{13}{|c}|}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
			%%		\toprule
			\hline
			%%				\multicolumn{13}{|c|}{12-dimensional tensors with $4^{12}$ elements}\\ \hline
			\multicolumn{14}{|c|}{Prescribed accuracy = $10^{-3}$}\\ \hline
			Function & TSize & \multicolumn{2}{|c|}{TT} & \multicolumn{4}{|c|}{H1}& \multicolumn{4}{|c|}{H2} & \multicolumn{2}{|c|}{H3}\\ \hline
			{} & {} & \multicolumn{2}{|c|}{} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{} \\ \hline
			{} & {} & Com(\%) & error & Com(\%) & error 
			& Com(\%) & error & Com(\%) & error
			& Com(\%) & error & Com(\%) & error\\ \hline
			1-1-1 & $7^4$  & 46.9388 & 1.9694e-04 & 46.9388 & 1.9694e-04 & 46.9388 & 1.9694e-04 & 46.9388 & 1.9694e-04 & 46.9388 & 1.9694e-04 & 46.9388 & 1.9694e-04 \\ \hline 
			1-1-2 & $7^4$  & 46.9388 & 2.0476e-04 & 46.9388 & 2.0476e-04 & 46.9388 & 2.0476e-04 & 46.9388 & 2.0476e-04 & 46.9388 & 2.0476e-04 & 46.9388 & 2.0476e-04 \\ \hline 
			1-2   & $25^4$ & 88.8000 & 4.8189e-04 & 88.8000 & 4.8189e-04 & 88.8000 & 4.8189e-04 & 88.8000 & 4.8189e-04 & 88.8000 & 4.8189e-04 & 88.8000 & 4.8189e-04 \\ \hline 
			2-1-1 & $2^4$  & -50 & 2.6686e-04 & -50 & 2.6686e-04 & -50 & 2.6686e-04 & -50 & 2.6686e-04 & -50 & 2.6686e-04 & -50 & 2.6686e-04 \\ \hline 
			2-2   & $10^4$ & 64 & 2.5359e-04 & 64 & 2.5359e-04 & 64 & 2.5359e-04 & 64 & 2.5359e-04 & 64 & 2.5359e-04 & 64 & 2.5359e-04 \\ \hline 
			3-1   & $35^4$ & 92 & 5.5155e-04 & 92 & 5.5155e-04 & 92 & 5.5155e-04 & 92 & 5.5155e-04 & 92 & 5.5155e-04 & 92 & 5.5155e-04 \\ \hline 
			%%		{}   & EF   & CHF    & EF2   & CHF2\\
			%%		1   &  17.5 & 19.1   & 17.5  & 19.1\\
			%%		2   &  11.8 & 12.7   & 29.3  & 31.9\\
			%%		3   &  6.6  &  5.6   & 35.9  & 37.4\\
			%%		\bottomrule
		\end{tabular}
	}
	\caption{Comparison of different heuristics for Chemistry data with prescribed accuracy of $10^{-3}$.\label{tab:chem-10-3}}
	%%		\end{minipage}
\end{table}

\begin{table}[!htb]
	\centering\tiny
	\rotatebox{0}{
		%%		\begin{minipage}{\textheight}
		%%\begin{tabular}{*{13}{|c}|}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
			%%		\toprule
			\hline
			%%				\multicolumn{13}{|c|}{12-dimensional tensors with $4^{12}$ elements}\\ \hline
			\multicolumn{14}{|c|}{Prescribed accuracy = $10^{-6}$}\\ \hline
			Function & TSize & \multicolumn{2}{|c|}{TT} & \multicolumn{4}{|c|}{H1}& \multicolumn{4}{|c|}{H2} & \multicolumn{2}{|c|}{H3}\\ \hline
			{} & {} & \multicolumn{2}{|c|}{} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{NG} & \multicolumn{2}{|c|}{G} & \multicolumn{2}{|c|}{} \\ \hline
			{} & {} & Com(\%) & error & Com(\%) & error 
			& Com(\%) & error & Com(\%) & error
			& Com(\%) & error & Com(\%) & error\\ \hline
			1-1-1 & $7^4$  & 6.1224 & 5.4857e-07 & 6.1224 & 5.4857e-07 & 6.1224 & 5.4857e-07 & 6.1224 & 5.4857e-07 & 6.1224 & 5.4857e-07 & 6.1224 & 5.4857e-07 \\ \hline 
			1-1-2 & $7^4$  & 2.0408 & 1.0727e-07 & 2.0408 & 1.0727e-07 & 2.0408 & 1.0727e-07 & 2.0408 & 1.0727e-07 & 2.0408 & 1.0727e-07 & 2.0408 & 1.0727e-07 \\ \hline 
			1-2   & $25^4$ & 73.7600 & 5.6886e-07 & 73.7600 & 5.6886e-07 & 73.7600 & 5.6886e-07 & 73.7600 & 5.6886e-07 & 73.7600 & 5.6886e-07 & 73.7600 & 5.6886e-07 \\ \hline 
			2-1-1 & $2^4$  & -100 & 5.9400e-16 & -100 & 6.3777e-16 & -100 & 6.3777e-16 & -100 & 7.6818e-16 & -100 & 7.6818e-16 & -100 & 5.0573e-16 \\ \hline 
			2-2   & $10^4$ & 32.0000 & 4.5248e-07 & 32.0000 & 4.5248e-07 & 32.0000 & 4.5248e-07 & 32.0000 & 4.5248e-07 & 32.0000 & 4.5248e-07 & 32.0000 & 4.5248e-07 \\ \hline 
			3-1   & $35^4$ & 78.7755 & 5.5212e-07 & 78.7755 & 5.5212e-07 & 78.7755 & 5.5212e-07 & 78.7755 & 5.5212e-07 & 78.7755 & 5.5212e-07 & 78.7755 & 5.5212e-07 \\ \hline 
			%%		{}   & EF   & CHF    & EF2   & CHF2\\
			%%		1   &  17.5 & 19.1   & 17.5  & 19.1\\
			%%		2   &  11.8 & 12.7   & 29.3  & 31.9\\
			%%		3   &  6.6  &  5.6   & 35.9  & 37.4\\
			%%		\bottomrule
		\end{tabular}
	}
	\caption{Comparison of different heuristics for Chemistry data with prescribed accuracy of $10^{-6}$.\label{tab:chem-10-6}}
	%%		\end{minipage}
\end{table}

\section{Discussion}
\label{sec:discussion}
\begin{itemize}
	\item Implementation of first svd computation
	\item Complexity and storage of the extra buffer in H3 heuristic
	\item Other decomposition instead of svd
	\item Analysis of operations in both algorithms
\end{itemize}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
 \bibliographystyle{splncs04}
 \bibliography{paralleltt}

\end{document}
